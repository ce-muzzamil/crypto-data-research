{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import *\n",
    "from neural_network import *\n",
    "import time\n",
    "import json\n",
    "from dataset import Dataset\n",
    "from rise_and_fall import *\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy as sci\n",
    "np.set_printoptions(precision=3, linewidth=2*75, suppress=False)\n",
    "import os\n",
    "import glob\n",
    "import shutil"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLOT FUNTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_frame(f: pd.DataFrame, mxis=[], mnis=[], mark=None):\n",
    "    ps = f[\"meanp\"].to_numpy()\n",
    "    mn = ps.min()\n",
    "    mx = ps.max()\n",
    "    plt.plot(ps)\n",
    "    if len(mxis) > 0:\n",
    "        plt.vlines(x=mxis, ymin=mn, ymax=mx, colors='green',)\n",
    "    elif 'mxis' in f.columns:\n",
    "        plt.vlines(x=f.index[f['mxis'] > 0], ymin=mn, ymax=mx, colors='green',)\n",
    "    if len(mnis) > 0:\n",
    "        plt.vlines(x=mnis, ymin=mn, ymax=mx, colors='red',)\n",
    "    elif 'mnis' in f.columns:\n",
    "        plt.vlines(x=f.index[f['mnis'] > 0], ymin=mn, ymax=mx, colors='red',)\n",
    "    if mark is not None:\n",
    "        plt.vlines(x=[mark], ymin=mn, ymax=mx, colors='blue',)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def unique(xs):\n",
    "    ys = []\n",
    "    for x in xs:\n",
    "        if x in ys:\n",
    "            continue\n",
    "        ys.append(x)\n",
    "    return ys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(symbol, show=True):\n",
    "    plt.figure(figsize=(15, 4))\n",
    "    mtypes = ['ch', 'bs', 'p']\n",
    "    yps=[]\n",
    "    yas=[]\n",
    "    r2s = []\n",
    "    for mt in mtypes:\n",
    "        model = get_bp_model(symbol, 3e-4, mt, allow_base=False)\n",
    "        _,_,data = normalized_data(symbol, mt)\n",
    "        yp = model.predict(data[:2], batch_size=512, verbose=0)\n",
    "        yps.extend(yp) if type(yp)==list else yps.append(yp)\n",
    "        ya = data[2:]\n",
    "        yas.extend(ya) if type(ya)==list else yas.append(ya)\n",
    "    \n",
    "    for i in range(4):\n",
    "        slope, intercept, r_value, p_value, std_err = sci.stats.linregress(yas[i], yps[i].flatten())\n",
    "        plt.subplot(1, len(yas), i+1)\n",
    "        plt.plot(yas[i], slope*yas[i]+intercept, c='r')\n",
    "        plt.text(0.0, 0.0, f'R2: {np.round(r_value**2, 2)}')\n",
    "        plt.scatter(yas[i], yps[i])\n",
    "        r2s.append(r_value**2)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        return r2s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INDICES CREATION FOR TRAINING VALIDATION AND TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('database/channels.json') as file:\n",
    "#     symbols = json.load(file)\n",
    "\n",
    "# indices = []\n",
    "# for symbol in symbols:\n",
    "#     df = pd.read_csv(f\"database/{symbol}.csv\")\n",
    "#     limit = df.shape[0]-1000\n",
    "#     low = 1000\n",
    "#     s,v = (int(limit*frac) for frac in [5/100, 10/100])\n",
    "#     r = limit-s-v\n",
    "#     s_ = np.random.randint(low, limit, size=s)\n",
    "\n",
    "#     v_ = np.random.randint(low, limit, size=v)\n",
    "#     similarity = np.isin(v_, s_)\n",
    "#     while np.any(similarity):\n",
    "#         v_[similarity] = np.random.randint(low, limit, v_[similarity].shape[0])\n",
    "#         similarity = np.isin(v_, s_)\n",
    "    \n",
    "#     r_ = np.random.randint(low, limit, size=r)\n",
    "#     similarity = np.isin(r_, np.append(v_, s_))\n",
    "#     while np.any(similarity):\n",
    "#         r_[similarity] = np.random.randint(low, limit, r_[similarity].shape[0])\n",
    "#         similarity = np.isin(r_, np.append(v_, s_))\n",
    "    \n",
    "#     indices.append(\n",
    "#         {\n",
    "#             'symbol': symbol,\n",
    "#             'r': r_,\n",
    "#             's': s_,\n",
    "#             'v': v_\n",
    "#         }\n",
    "#     )\n",
    "\n",
    "# np.savez_compressed('database/indices.npz', indices=indices)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAVING DATASETS IN NPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('database/channels.json') as file:\n",
    "    symbols = json.load(file)\n",
    "\n",
    "def save_set(x1, itter, symbol, sc):\n",
    "    np.save(f'database/buy/{symbol}/{sc}x1.npy', x1)\n",
    "    x2, y1, y2, y3, y4 = itter\n",
    "    np.save(f'database/buy/{symbol}/{sc}x2.npy', x2)\n",
    "    np.save(f'database/buy/{symbol}/{sc}y1.npy', y1)\n",
    "    np.save(f'database/buy/{symbol}/{sc}y2.npy', y2)\n",
    "    np.save(f'database/buy/{symbol}/{sc}y3.npy', y3)\n",
    "    np.save(f'database/buy/{symbol}/{sc}y4.npy', y4)\n",
    "\n",
    "dataset_size = 350000\n",
    "trx1 = []; trothers = []\n",
    "vlx1 = []; vlothers = []\n",
    "tsx1 = []; tsothers = []\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for symbol in symbols:\n",
    "        if symbol in [r.split('\\\\')[-1] for r in glob.glob('database/buy/*') if '.' not in r]:\n",
    "            continue\n",
    "        print(symbol, ' started')\n",
    "        db = Dataset(symbol)\n",
    "        x1, others = db.get_data_set(\n",
    "            batch_size=dataset_size, max_procs=4, single_process_size=int(dataset_size/4), buy=True, validation=False, test=False)\n",
    "\n",
    "        norm = x1.max(axis=(0,1))\n",
    "        norm[-1] = 288.0\n",
    "        if not os.path.isdir(f'database/buy/{symbol}'):\n",
    "            os.mkdir(f'database/buy/{symbol}')\n",
    "        np.save(f'database/buy/{symbol}/norm.npy', norm)\n",
    "        save_set(x1, others, symbol, 'tr')\n",
    "\n",
    "        time.sleep(3*60)\n",
    "\n",
    "        x1, others = db.get_data_set(\n",
    "            batch_size=2**12, max_procs=8, single_process_size=512, buy=True, validation=True, test=False)\n",
    "        save_set(x1, others, symbol, 'vl')\n",
    "\n",
    "        x1, others = db.get_data_set(\n",
    "            batch_size=2**12, max_procs=8, single_process_size=512, buy=True, validation=False, test=True)\n",
    "        save_set(x1, others, symbol, 'ts')\n",
    "\n",
    "        time.sleep(5*60)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('database/channels.json') as file:\n",
    "    symbols = json.load(file)\n",
    "\n",
    "def save_set(x1, itter, symbol, sc):\n",
    "    np.save(f'database/sell/{symbol}/{sc}x1.npy', x1)\n",
    "    x2, y1, y2 = itter\n",
    "    np.save(f'database/sell/{symbol}/{sc}x2.npy', x2)\n",
    "    np.save(f'database/sell/{symbol}/{sc}y1.npy', y1)\n",
    "    np.save(f'database/sell/{symbol}/{sc}y2.npy', y2)\n",
    "\n",
    "dataset_size = 64000\n",
    "trx1 = []; trothers = []\n",
    "vlx1 = []; vlothers = []\n",
    "tsx1 = []; tsothers = []\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for symbol in symbols:\n",
    "        if symbol in [r.split('\\\\')[-1] for r in glob.glob('database/sell/*') if '.' not in r]:\n",
    "            continue\n",
    "        print(symbol, ' started')\n",
    "        db = Dataset(symbol)\n",
    "        x1, others = db.get_data_set(\n",
    "            batch_size=dataset_size, max_procs=4, single_process_size=int(dataset_size/4), buy=False, validation=False, test=False)\n",
    "\n",
    "        norm = x1.max(axis=(0,1))\n",
    "        norm[-1] = 288.0\n",
    "        if not os.path.isdir(f'database/sell/{symbol}'):\n",
    "            os.mkdir(f'database/sell/{symbol}')\n",
    "        np.save(f'database/sell/{symbol}/norm.npy', norm)\n",
    "        save_set(x1, others, symbol, 'tr')\n",
    "\n",
    "        # time.sleep(3*60)\n",
    "\n",
    "        x1, others = db.get_data_set(\n",
    "            batch_size=2**12, max_procs=8, single_process_size=512, buy=False, validation=True, test=False)\n",
    "        save_set(x1, others, symbol, 'vl')\n",
    "\n",
    "        x1, others = db.get_data_set(\n",
    "            batch_size=2**12, max_procs=8, single_process_size=512, buy=False, validation=False, test=True)\n",
    "        save_set(x1, others, symbol, 'ts')\n",
    "\n",
    "        time.sleep(3*60)\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for symbol in [r.split('\\\\')[-1] for r in glob.glob('database/buy/*') if '.' not in r]:\n",
    "    try:\n",
    "        print(symbol)\n",
    "        test(symbol)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RANDOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = buy_predictor(finetune=True)\n",
    "# plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Dataset(\"BTCUSDT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregated_minimalistic_frame(db, i, mf, at, kinterval=5.0):\n",
    "    '''i is the last index of minimalistic frame (index of sell), mf is the minimalistic frame, at is the index for agregation'''\n",
    "    if mf.shape[0]==0:\n",
    "        raise Exception('Invalid mf')\n",
    "    to_be_aggreagted = db.nf.loc[i:at, :]\n",
    "    vector = np.zeros((mf.shape[1],))\n",
    "    ft = db.nf.iloc[i: at, :]\n",
    "    vector[[0, 1]] = ft.loc[ft.index[-1], ['meanp', 'stdp']]\n",
    "    vector[[3, 5, 6]] = ft.loc[:, ['meanp', 'taker', 'maker']].mean()\n",
    "    meanpstd = ft.loc[:, 'meanp'].std()\n",
    "    if np.isnan(meanpstd):\n",
    "        meanpstd = 0.0\n",
    "    vector[4] = meanpstd\n",
    "    vector[[7, 8]] = ft.loc[:, ['vol', 'ntrds']].sum()\n",
    "    vector[9] = ft.shape[0]*kinterval\n",
    "    ctime = pd.to_datetime(ft['stime'].iloc[-1], unit='ms')\n",
    "\n",
    "    a = np.concatenate([mf, vector.reshape(1, -1)], axis=0)\n",
    "    ctime = ctime.weekday()*288 + (ctime.hour*60.0 + ctime.minute)/5.0\n",
    "    return a, ctime\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "at = 80000\n",
    "max_size=64\n",
    "kinterval=5.0\n",
    "a, x, n = db.get_minimalistic_frame(at=at, max_size=64*2)\n",
    "xnd = []\n",
    "for i in x:\n",
    "    xnd.append({\n",
    "        'index': i,\n",
    "        'type': 1\n",
    "    })\n",
    "for i in n:\n",
    "    xnd.append({\n",
    "        'index': i,\n",
    "        'type': 0\n",
    "    })\n",
    "xnf = pd.DataFrame(xnd)\n",
    "xnf.sort_values('index', ignore_index=True, inplace=True)\n",
    "xnf = xnf[-max_size*2:].reset_index(drop=True)\n",
    "\n",
    "inputs=[]\n",
    "outputs=[]\n",
    "\n",
    "for i in range(max_size-1, max_size*2-2):\n",
    "    ji=xnf['index'][i]+1\n",
    "    jf=xnf['index'][i+1]\n",
    "    j=np.random.randint(ji, jf+1)\n",
    "    ins = aggregated_minimalistic_frame(db, xnf['index'][i], a[(i-max_size+1):i], j); inputs.append(ins)\n",
    "    if xnf['type'][i]==0:\n",
    "        #'0 means i is mni' mean its bullish\n",
    "        buy = (xnf['index'][i+2]-j)*kinterval\n",
    "        sell = (xnf['index'][i+1]-j)*kinterval\n",
    "        #last and future prices are correct think before change\n",
    "        future_price = db.nf.loc[xnf['index'][i+1], 'meanp']\n",
    "        last_price = db.nf.loc[xnf['index'][i], 'meanp']\n",
    "    else:\n",
    "        #'1 means i is mxi' mean its bearish\n",
    "        buy = (xnf['index'][i+1]-j)*kinterval\n",
    "        sell = (xnf['index'][i+2]-j)*kinterval\n",
    "        #last and future prices are correct think before change\n",
    "        future_price = db.nf.loc[xnf['index'][i+2], 'meanp']\n",
    "        last_price = db.nf.loc[xnf['index'][i+1], 'meanp']\n",
    "\n",
    "    ch = (future_price - last_price)/last_price\n",
    "    outputs.append([buy, future_price, sell, ch*100.0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans='sell'\n",
    "for symbol in [r.split('\\\\')[-1] for r in glob.glob(f'database/{trans}/*') if '.' not in r]:\n",
    "    for mtype in ['s', 'p']:\n",
    "        # reset_history(symbol=symbol, mtype=mtype, trans=trans, new_name=\"SBASE\")\n",
    "        with open(f'database/{trans}/{symbol}/{mtype}_history/version_history.json', 'r') as file:\n",
    "            d = json.load(file)\n",
    "\n",
    "        d[-1][\"id\"] = \"SBASE\"\n",
    "        with open(f'database/{trans}/{symbol}/{mtype}_history/version_history.json', 'w') as file:\n",
    "            json.dump(d, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans='buy'\n",
    "for symbol in [r.split('\\\\')[-1] for r in glob.glob(f'database/{trans}/*') if '.' not in r]:\n",
    "    for mtype in ['ch', 'b', 's', 'p']:\n",
    "        reset_history(symbol=symbol, mtype=mtype, trans=trans, new_name=\"Base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans='sell'\n",
    "for symbol in [r.split('\\\\')[-1] for r in glob.glob(f'database/{trans}/*') if '.' not in r]:\n",
    "    for mtype in ['s', 'p']:\n",
    "        reset_history(symbol=symbol, mtype=mtype, trans=trans, new_name=\"Base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dat={\n",
    "    \"id\": \"Base\",\n",
    "    \"time\": 1672842686.6763935,\n",
    "    \"loss\": 0.094559235792411,\n",
    "    \"y1_loss\": 0.04415296723968104,\n",
    "    \"y3_loss\": 0.050406275022971,\n",
    "    \"val_loss\": 0.09608950583558333,\n",
    "    \"val_y1_loss\": 0.04403252115375117,\n",
    "    \"val_y3_loss\": 0.05205698625037545,\n",
    "    \"lr\": 1,\n",
    "    \"epochs\": 19\n",
    "}\n",
    "\n",
    "for symbol in [r.split('\\\\')[-1] for r in glob.glob(f'database/{trans}/*') if '.' not in r]:\n",
    "    trans='buy'\n",
    "    mtype = 'bs'\n",
    "    model = get_bp_model(symbol=symbol, lr=3e-4, mtype=mtype)\n",
    "\n",
    "    bmodel = b_predictor()\n",
    "    smodel = s_predictor()\n",
    "    for i in range(5):\n",
    "        smodel.layers[i].set_weights(model.layers[i].weights)\n",
    "    smodel.layers[5].set_weights(model.layers[6].weights)\n",
    "    smodel.layers[6].set_weights(model.layers[5].weights)\n",
    "    for i in range(7,10):\n",
    "        smodel.layers[i].set_weights(model.layers[i].weights)\n",
    "    smodel.layers[10].set_weights(model.layers[11].weights)\n",
    "    smodel.layers[11].set_weights(model.layers[10].weights)\n",
    "    for i in range(12,len(model.layers)):\n",
    "        smodel.layers[i].set_weights(model.layers[i].weights)\n",
    "\n",
    "    for i in range(11):\n",
    "        bmodel.layers[i].set_weights(model.layers[i].weights)\n",
    "\n",
    "    os.mkdir(f\"database/{trans}/{symbol}/b_history\")\n",
    "    bmodel.save_weights(f\"database/{trans}/{symbol}/b_history/Base.h5\")\n",
    "    with open(f\"database/{trans}/{symbol}/b_history/version_history.json\", 'w') as file:\n",
    "        json.dump([temp_dat], file)\n",
    "    os.mkdir(f\"database/{trans}/{symbol}/s_history\")\n",
    "    smodel.save_weights(f\"database/{trans}/{symbol}/s_history/Base.h5\")\n",
    "    with open(f\"database/{trans}/{symbol}/s_history/version_history.json\", 'w') as file:\n",
    "        json.dump([temp_dat], file)\n",
    "\n",
    "for symbol in [r.split('\\\\')[-1] for r in glob.glob(f'database/buy/*') if '.' not in r]:\n",
    "    shutil.rmtree(f\"database/buy/{symbol}/bs_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for symbol in [r.split('\\\\')[-1] for r in glob.glob(f'database/{trans}/*') if '.' not in r]:\n",
    "    for mtype in ['s', 'p']:\n",
    "        if not os.path.isdir(f\"database/sell/{symbol}/{mtype}_history\"):\n",
    "            shutil.copytree(f\"database/buy/{symbol}/{mtype}_history\", f\"database/sell/{symbol}/{mtype}_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sym = \"BTCUSDT\"\n",
    "mtype = 's'\n",
    "trans='buy'\n",
    "s = normalized_test_data(sym, mtype, trans=trans)\n",
    "model = get_bp_model(sym, 3e-4, mtype, trans=trans)\n",
    "with tf.device('/cpu:0'):\n",
    "    yp = model.predict(s[:2], batch_size=4096).flatten()\n",
    "plt.scatter(s[2], yp)\n",
    "slope, intercept, r_value, p_value, std_err = sci.stats.linregress(s[2], yp)\n",
    "plt.plot(s[2], slope*s[2]+intercept, c='r')\n",
    "plt.text(0.0, 0.0, f'R2: {np.round(r_value**2, 2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[r.split('\\\\')[-1] for r in glob.glob(f'database/{trans}/*') if '.' not in r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "sets = []\n",
    "trans='buy'\n",
    "for symbol in [r.split('\\\\')[-1] for r in glob.glob(f'database/{trans}/*') if '.' not in r]:\n",
    "    for mtype in ['ch', 'p', 'b', 's']:\n",
    "        with tf.device('/cpu:0'):\n",
    "            model = get_bp_model(symbol, lr=3e-4, mtype=mtype, trans=trans)\n",
    "            s = normalized_test_data(symbol, mtype, trans=trans)\n",
    "            models.append(model)\n",
    "            sets.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = [r.split('\\\\')[-1] for r in glob.glob(f'database/{trans}/*') if '.' not in r]\n",
    "print(\"plotting\")\n",
    "j=0\n",
    "plt.figure(figsize=(4*4, 11*4))\n",
    "for i in range(0, len(models)):\n",
    "    # with tf.device('/cpu:0'):\n",
    "    yp = models[i].predict(sets[i][:2], batch_size=728).flatten()\n",
    "    plt.subplot(11, 4, i+1)\n",
    "    plt.scatter(sets[i][2], yp, label=f'{symbols[j]}')\n",
    "    slope, intercept, r_value, p_value, std_err = sci.stats.linregress(sets[i][2], yp)\n",
    "    plt.plot(sets[i][2], slope*sets[i][2]+intercept, c='r')\n",
    "    plt.text(0.0, 0.0, f'R2: {np.round(r_value**2, 2)}')\n",
    "    plt.legend()\n",
    "    if i!=0 and (i+1)%4==0:\n",
    "        j+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "19d1d53a962d236aa061289c2ac16dc8e6d9648c89fe79f459ae9a3493bc67b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
