{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "# from keras.utils.vis_utils import plot_model\n",
    "from matplotlib import pyplot as plt\n",
    "np.set_printoptions(precision=3, linewidth=2*75, suppress=False)\n",
    "from IPython.display import clear_output\n",
    "from rise_and_fall import *\n",
    "from dataset import Dataset\n",
    "import json\n",
    "import time\n",
    "from neural_network import *\n",
    "from main import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLOT FUNTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_frame(f:pd.DataFrame, mxis=[], mnis=[], mark=None):\n",
    "    ps = f[\"meanp\"].to_numpy()\n",
    "    mn = ps.min()\n",
    "    mx = ps.max()\n",
    "    plt.plot(ps)\n",
    "    if len(mxis)>0:\n",
    "        plt.vlines(x = mxis, ymin = mn, ymax = mx, colors = 'green',)\n",
    "    elif 'mxis' in f.columns:\n",
    "        plt.vlines(x = f.index[f['mxis']>0], ymin = mn, ymax = mx, colors = 'green',)\n",
    "    if len(mnis)>0:\n",
    "        plt.vlines(x = mnis, ymin = mn, ymax = mx, colors = 'red',)\n",
    "    elif 'mnis' in f.columns:\n",
    "        plt.vlines(x = f.index[f['mnis']>0], ymin = mn, ymax = mx, colors = 'red',)\n",
    "    if mark is not None:\n",
    "        plt.vlines(x = [mark], ymin = mn, ymax = mx, colors = 'blue',)\n",
    "    plt.show()\n",
    "    \n",
    "def unique(xs):\n",
    "    ys = []\n",
    "    for x in xs:\n",
    "        if x in ys:\n",
    "            continue\n",
    "        ys.append(x)\n",
    "    return ys"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAVING DATASETS IN NPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db = Dataset()\n",
    "# if __name__=='__main__':\n",
    "#     inputs, outputs = db.get_data_set(\n",
    "#             batch_size=2**14, max_procs=10, single_process_size=2**10, buy=True, validation=False, test=False)\n",
    "\n",
    "#     vlx, vly = db.get_data_set(\n",
    "#             batch_size=2**11, max_procs=10, single_process_size=2**10, buy=True, validation=True, test=False)\n",
    "\n",
    "# #     lrx, lry = db.get_data_set(\n",
    "# #             batch_size=2**10, max_procs=10, single_process_size=2**9, buy=True, validation=False, test=False)\n",
    "\n",
    "# np.save('database/inputs.npy', inputs)\n",
    "# np.save('database/outputs.npy', outputs)\n",
    "\n",
    "# np.save('database/vlx.npy', vlx)\n",
    "# np.save('database/vly.npy', vly)\n",
    "\n",
    "# np.save('database/lrx.npy', lrx)\n",
    "# np.save('database/lry.npy', lry)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RETRIVING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsx_name = 'vlx'\n",
    "dsy_name = 'vly'\n",
    "\n",
    "ds = np.load(f'database/{dsx_name}.npy', mmap_mode='r')\n",
    "indicies = np.random.choice(ds.shape[0], (2**10,))\n",
    "x = ds[indicies]\n",
    "del ds\n",
    "print(x.shape)\n",
    "\n",
    "ds = np.load(f'database/{dsy_name}.npy', mmap_mode='r')\n",
    "y = ds.T[indicies]\n",
    "y=y.T\n",
    "del ds\n",
    "print(y.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRETRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated New Model\n",
      "Train on 16384 samples, validate on 2048 samples\n",
      "Epoch 1/5\n",
      "16384/16384 [==============================] - 240s 15ms/sample - loss: 1910.6971 - y1_loss: 952.8832 - y2_loss: 943.6972 - y3_loss: 14.1167 - val_loss: 3095.9038 - val_y1_loss: 1000.0000 - val_y2_loss: 1000.0000 - val_y3_loss: 1095.9038\n",
      "Epoch 2/5\n",
      "12800/16384 [======================>.......] - ETA: 47s - loss: 1944.8287 - y1_loss: 941.5505 - y2_loss: 999.3104 - y3_loss: 3.9677"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8560\\495358600.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.05\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m train_bp_model(inputs=inputs, outputs=outputs,\n\u001b[1;32m----> 8\u001b[1;33m                        validation_data=(vlx, vly), epochs=5, verbose=1, lr=lr, mini_batch_size=512, patience=2)\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# model = get_bp_model(lr)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\noble\\Desktop\\TimePass\\crypto-data-research\\neural_network.py\u001b[0m in \u001b[0;36mtrain_bp_model\u001b[1;34m(inputs, outputs, validation_data, verbose, epochs, lr, save, patience, mini_batch_size)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m     hist = model.fit(inputs, outputs, epochs=epochs,\n\u001b[1;32m--> 156\u001b[1;33m                      validation_data=validation_data, verbose=verbose, callbacks=[callback], batch_size=mini_batch_size)\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Neural network trained in {format_time(time.time()-st)}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    729\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 675\u001b[1;33m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    676\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3476\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1472\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1473\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "inputs, outputs = get_data('inputs', 'outputs', 2**14)\n",
    "vlx, vly = get_data('vlx', 'vly', 2**11)\n",
    "lrx, lry = get_data('inputs', 'outputs', 2**10)\n",
    "\n",
    "# lr = get_op_lr(lrx=lrx, lry=lry, mini_batch_size=32)\n",
    "lr = 0.05\n",
    "train_bp_model(inputs=inputs, outputs=outputs,\n",
    "                       validation_data=(vlx, vly), epochs=5, verbose=1, lr=lr, mini_batch_size=512, patience=2)\n",
    "\n",
    "# model = get_bp_model(lr)\n",
    "# hist = model.fit(inputs, outputs, validation_data=(vlx,vly), epochs=5, batch_size=256)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_bp_model(3e-4)\n",
    "yp = model.predict(x)\n",
    "ya = [y[i] for i in range(y.shape[0])]\n",
    "plt.figure(figsize=(15,4))\n",
    "for i in range(y.shape[0]):\n",
    "    plt.subplot(1,3,i+1)\n",
    "    plt.scatter(ya[i], yp[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RANDOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_bp_model(1e-3)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.constant([10.0 for _ in range(10)])\n",
    "x.get_shape().as_list()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.constant([i for i in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasplit.json\", 'r') as file:\n",
    "        dataset = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c,d = db.labeled_buy_frame(at=100000-288, length=288, to_numpy=True)\n",
    "display(a.shape)\n",
    "# plot_frame(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = []\n",
    "size = 0\n",
    "look_past = 0\n",
    "while size<32:\n",
    "    a,b,c,d = db.labeled_buy_frame(at=100000-look_past, length=288, to_numpy=False)\n",
    "    ai = a.loc[np.logical_or(a['mnis']==1, a['mxis']==1), :]\n",
    "    temp_a = np.zeros((ai.shape[0], 7))\n",
    "    for i in range(1, ai.index.shape[0]):\n",
    "        temp_a[i, 0:4] = a.loc[ai.index[i-1]:ai.index[i-1], ['meanp', 'ch', 'taker', 'maker']].mean().to_numpy().flatten()\n",
    "        temp_a[i, 4:6] = a.loc[ai.index[i-1]:ai.index[i-1], ['vol', 'ntrds']].sum().to_numpy().flatten()\n",
    "        temp_a[i, 6] = a.loc[ai.index[i-1]:ai.index[i-1], 'meanp'].std()\n",
    "    \n",
    "    temp_a =pd.DataFrame(temp_a, columns=['meanpp', 'chp', 'takerp', 'makerp','vols', 'ntrdss', 'meanpsstd'])\n",
    "    ai.reset_index(drop=True, inplace=True)\n",
    "    pd.concat([ai, temp_a], axis=1)\n",
    "\n",
    "    aa.append(a)\n",
    "    size += a.shape[0]\n",
    "    look_past+=288\n",
    "aa.reverse()\n",
    "a = pd.concat(aa)\n",
    "a=a.iloc[-32:]\n",
    "a.reset_index(drop=True, inplace=True)\n",
    "day = a.pop('day')\n",
    "a['stime'] = a['stime']+day*18.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c,d = db.labeled_buy_frame(at=100000-look_past, length=288, to_numpy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = a.to_numpy().mean()\n",
    "sigma = a.to_numpy().std()\n",
    "(a-mu)/sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.loc[6:10, 'meanp'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_frame(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_learning_rate = 0.005\n",
    "# decay_rate=0.87\n",
    "# decay_steps=1024*5\n",
    "# y = lambda step: initial_learning_rate * (decay_rate ** (np.round(step / decay_steps, 0)))\n",
    "\n",
    "# steps = np.array([i for i in range(int(100*1024))])\n",
    "# plt.semilogy(steps, y(steps))\n",
    "# print(y(steps)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_bp_model(1).summary())\n",
    "lrs = lambda epoch: 1e-4 * 10**(epoch/20)\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lrs)\n",
    "history = train_bp_model(inputs=x, outputs=[y[i] for i in range(y.shape[0])], epochs=60, verbose=0, save=False, patience=20, mini_batch_size=128)\n",
    "losses = history.history['loss']\n",
    "epochs = np.array([i for i in range(len(losses))])\n",
    "plt.semilogx(lrs(epochs), history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs(epochs)[np.argmin(losses)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda step: initial_learning_rate * (decay_rate ** (np.round(step / decay_steps, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array([i*np.log(i+1) for i in range(100)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=0.1\n",
    "ys = []\n",
    "for i in range(100):\n",
    "    ys.append(y)\n",
    "    # y = y*0.97**(i**2.6/(i+1)**2.4)\n",
    "    y = 0.05*0.9**(12*np.log(i+1))\n",
    "print(ys[-1])\n",
    "plt.plot(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i**2.0 for i in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([64*np.log(i+1) for i in range(63)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([2*np.exp(i**(0.95**i)) for i in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i*18.0 for i in range(7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = buy_predictor()\n",
    "# plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "# model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "19d1d53a962d236aa061289c2ac16dc8e6d9648c89fe79f459ae9a3493bc67b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
